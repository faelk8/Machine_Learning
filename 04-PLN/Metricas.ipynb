{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install evaluate rouge_score transformers torch\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy and F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6442405679e4cde87a4756403416a31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b65f383a42a44c9a0956188f1a1dfd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: {'accuracy': 1.0}\n",
      "F1 Score: {'f1': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Sample dataset about Japanese tea ceremony\n",
    "references = [\n",
    "    \"The Japanese tea ceremony is a profound cultural practice emphasizing harmony and respect.\",\n",
    "    \"Matcha is carefully prepared using traditional methods in a tea ceremony.\",\n",
    "    \"The tea master meticulously follows precise steps during the ritual.\"\n",
    "]\n",
    "\n",
    "predictions = [\n",
    "    \"Japanese tea ceremony is a cultural practice of harmony and respect.\",\n",
    "    \"Matcha is prepared using traditional methods in tea ceremonies.\",\n",
    "    \"The tea master follows precise steps during the ritual.\"\n",
    "]\n",
    "\n",
    "# Accuracy and F1 Score\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "# Simulate binary classification (e.g., ceremony vs. non-ceremony)\n",
    "labels = [1, 1, 1]  # All are about tea ceremony\n",
    "pred_labels = [1, 1, 1]  # Model predicts all correctly\n",
    "\n",
    "accuracy = accuracy_metric.compute(predictions=pred_labels, references=labels)\n",
    "f1 = f1_metric.compute(predictions=pred_labels, references=labels, average='weighted')\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70e1e58151644acfb3d279c7116f301d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/8.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5013ab36b39483c909e00f236b33f83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b023409a65485687b6d32e8b07ec76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7760e1f165ce45d98f812c2b41531d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba7be37d82e45e3b1dc8e8af5cf9070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a7bfdcf8f04aeeae86cbbdce2ec3ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70df5d7c5ea243f1af2d39637f6585e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e5fe9f53324504be770dd68f659ddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f61e43569f0d4cb4b9ef3ce8c76f2876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: {'perplexities': [115.2373046875, 324.3690185546875, 417.4598083496094], 'mean_perplexity': 285.68871053059894}\n"
     ]
    }
   ],
   "source": [
    "# Perplexity (using a small GPT2 language model)\n",
    "perplexity_metric = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "perplexity = perplexity_metric.compute(\n",
    "    predictions=predictions, \n",
    "    model_id='gpt2'  # Using a small pre-trained model\n",
    ")\n",
    "print(\"Perplexity:\", perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROUGE, BLEU and METEOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34a2f6943f3a47e4a0afcb4df81b9642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores: {'rouge1': 0.8602339181286549, 'rouge2': 0.6718162012279659, 'rougeL': 0.8602339181286549, 'rougeLsum': 0.8602339181286549}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebfb0c0d345a4aee8d544af1ce000bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab900b8313f74dcabd3b39a17c3571f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6228f2fa481b4b5f85e309be26f56dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: {'bleu': 0.5260257094802832, 'precisions': [0.9375, 0.7241379310344828, 0.5384615384615384, 0.391304347826087], 'brevity_penalty': 0.8553453273074225, 'length_ratio': 0.8648648648648649, 'translation_length': 32, 'reference_length': 37}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e827e033aaea4776a1d7de2310a529d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/rafael/nltk_data...\n",
      "[nltk_data] Downloading package punkt_tab to /home/rafael/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to /home/rafael/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METEOR Score: {'meteor': 0.8462650219142979}\n"
     ]
    }
   ],
   "source": [
    "# ROUGE Score (no LLM loaded, using pre-defined lists of texts as LLM outputs (predictions) and references)\n",
    "rouge_metric = evaluate.load('rouge')\n",
    "rouge_results = rouge_metric.compute(\n",
    "    predictions=predictions, \n",
    "    references=references\n",
    ")\n",
    "print(\"ROUGE Scores:\", rouge_results)\n",
    "\n",
    "# BLEU Score (no LLM loaded, using pre-defined lists of texts as LLM outputs (predictions) and references)\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "bleu_results = bleu_metric.compute(\n",
    "    predictions=predictions, \n",
    "    references=references\n",
    ")\n",
    "print(\"BLEU Score:\", bleu_results)\n",
    "\n",
    "# METEOR (requires references to be a list of lists)\n",
    "meteor_metric = evaluate.load(\"meteor\")\n",
    "meteor_results = meteor_metric.compute(\n",
    "    predictions=predictions, \n",
    "    references=[[ref] for ref in references]\n",
    ")\n",
    "print(\"METEOR Score:\", meteor_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Exact Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "# 6. Exact Match\n",
    "def exact_match_compute(predictions, references):\n",
    "    return sum(pred.strip() == ref.strip() for pred, ref in zip(predictions, references)) / len(predictions)\n",
    "\n",
    "em_score = exact_match_compute(predictions, references)\n",
    "print(\"Exact Match Score:\", em_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
